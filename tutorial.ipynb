{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a clickbait classification pipeline\n",
    "\n",
    "![Clickbait](http://www.ogilvy.com/wp-content/uploads/2015/10/166_main.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this workshop is to walk you through the process of taking data to a production machine learning classifier for text. You'll build a classifier to detect clickbait from the article text and eventually roll out the classifier into an app.\n",
    "\n",
    "We'll be using\n",
    " - pandas for data wrangling\n",
    " - matplotlib for plotting\n",
    " - scikit-learn for feature engineering, model building, and model analysis\n",
    " - flask for building our web app\n",
    " - jupyter for getting stuff done\n",
    "\n",
    "By the end of the workshop you'll understand the steps needed to build a basic text processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and wrangling\n",
    "\n",
    "You've been presented with tabular data in a `.csv` format. A natural choice for loading the data is pandas, which is based around tabular representations.\n",
    "\n",
    "We'll load in the data and get a broad overview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/training_data.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article details\n",
    "\n",
    "An article contains:\n",
    " - **author** (string) - the article author's name\n",
    " - **description** (string) - a short description of the article\n",
    " - **label** (interger) - 1=clickbait 0=not clickbait\n",
    " - **publishedAt** (string) - a timestamp for the time of publication\n",
    " - **title** (string) - the title of the article\n",
    "\n",
    "We can print the content of one article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['title'][1])\n",
    "print('-----')\n",
    "print(df['description'][1])\n",
    "print('Published at:', df['publishedAt'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic statistics\n",
    "\n",
    "Pandas makes it easy to inspect, plot, and transform our data. Pandas is easy to use but still has lots of functionality. When you can chain multiple functions together, you've become a pandas pro!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clickbait and non-clickbait articles\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the number of author fields that are Null\n",
    "df['author'].isnull().value_counts().plot('barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of characters in the description field\n",
    "df['description'].apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the number of description characters in clickbait to news\n",
    "df['description'].apply(len).groupby(df['label']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST YOUR KNOWLEDGE\n",
    "# Can you write a one-liner to compute the number of clickbait articles\n",
    "# written by each author? Hint: you might find the .sum() function helpful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the full content column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['full_content'] = df.description + ' ' + df.title\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice work - have a panda\n",
    "\n",
    "![panda](http://www.nathab.com/uploaded-files/carousels/TRIPS/Wild-China/Asia-Wild-China-4-panda.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Scikit-learn text classification pipeline\n",
    "\n",
    "\n",
    "Some important terminology:\n",
    "\n",
    "<img src=\"http://tfwiki.net/mediawiki/images2/thumb/3/37/Optimusg1.jpg/350px-Optimusg1.jpg\" alt=\"optimus\" style=\"width:50px;\" align=\"left\"/> **TRANSFORMERS** - take some input data and transform it into another format. Often we want to transform textual data or image data into numerical data. We may also transform our input data into new features\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<img src=\"http://www.kennyskiphire.co.uk/blog/wp-content/uploads/Wheelie-Bins.jpg\" alt=\"bins\" style=\"width:70px;\" align=\"left\"/> **CLASSIFIERS** - take some input data and classify the sample by assigning a label to the input data. In binary classification we often use the labels 1 and 0.\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<img src=\"https://reichanjapan.files.wordpress.com/2016/02/mariogiftcard.png?w=230&h=335\" alt=\"pipe\" style=\"width:50px;\" align=\"left\"/> **PIPELINE** - consist of one or many transformer steps followed by a classifier. We can use pipelines to elegantly chain together operations and construct an easy to use interface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual to numerical data (bag of words model) \n",
    "\n",
    "Our classifier isn't going to understand text like we can - we must create numerical data. A common approach to this is the bag of words model. \n",
    "\n",
    "For example - **Literally just 8 really really cute dogs** - transforms into the bag of words:\n",
    "\n",
    "| Token | id | Count |\n",
    "|---|---|---|\n",
    "| cute | 0 |1 |\n",
    "| dogs | 1 |1 |\n",
    "| just | 2 | 1 |\n",
    "| literally | 3 | 1 |\n",
    "| really | 4 | 2 |\n",
    "\n",
    "This is simply achieved with a scikit-learn `CountVectorizer`. There are two steps:\n",
    " - **Fit** the vectorizer, which populates all the tokens in the left hand column and assigns the numerical ids\n",
    " - **Transform** the data, which turns a sentence into it's bag of words representation\n",
    " \n",
    " Note that the bag of words representation of a sentence ignores the word order and dependencies between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the `CountVectorizer` learns the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"Literally just 8 really really cute dogs\"]\n",
    "vectorizer.fit(sentence)\n",
    "print(vectorizer.vocabulary_) # dictionary of words and ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then transform our textual to numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.transform(sentence).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we cannot transform textual data that is not in our learned vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"OMG 5 truly hilarious dogs ðŸ˜‚\"]\n",
    "vectorizer.transform(sentence).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "In the classification task, we take a single example (such as an article row) and decide which class it belongs to (e.g., clickbait or not clickbait). A standard approach to classification is to find a boundary that best seperates training examples according to their class. In the binary classification problem below, we've indicated a linear boundary that separates the data pretty well. Each sample is described by the number of times that word1 and word2 occur. In reality we will have many more words associated with each sample but the concept remains the same.\n",
    "\n",
    "We determine this boundary using a Support Vector Machine. The SVM has two steps:\n",
    " - **Fit** we learn the boundary from labelled data\n",
    " - **Predict** we predict the classes of unlabelled data\n",
    "\n",
    "![classify](images/svm-classify.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svc = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make up some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_of_words = [\n",
    "    [1, 5], [1, 4], [2, 6], [4, 2], [3, 4], [2, 1]\n",
    "]\n",
    "labels = [1, 1, 1, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import plot_2d_samples\n",
    "plot_2d_samples(bag_of_words, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we learn the boundary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = svc.fit(bag_of_words, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import plot_2d_trained_svc\n",
    "plot_2d_trained_svc(bag_of_words, labels, svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have learned the boundary then we can predict the label of novel samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.predict([[3, 1], [2,4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together in a pipeline\n",
    "\n",
    "A pipeline consists of multiple transform steps and a final classification step. A pipeline is an easy way to wrap up all our transformations in one easy to use box. In general we use the following functions:\n",
    " - **Fit** to fit the transformers and classifier\n",
    " - **Predict** to transform data and predict it's label\n",
    " \n",
    "Below is a detailed schematic of the data flow in the pipeline when we call these two methods.\n",
    "\n",
    "![pipeobj](images/sklearn-pipeline.png)\n",
    "\n",
    "We'll create a pipeline with two steps:\n",
    "1. Transform textual data to a bag of words vector\n",
    "2. Predict label from the bag of words vector\n",
    "\n",
    "So the input of our pipeline is text data and the output is a label!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps = (\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LinearSVC())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations!** You've built your first text classification pipeline.\n",
    "![pipeygif](https://tinynin.files.wordpress.com/2012/01/warppipe-copy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our pipeline on real data\n",
    "\n",
    "Now that we know how the vectorizer and classifier work together to form a pipeline, we can train it on the real data. \n",
    "\n",
    "**Machine learning discipline 101**\n",
    " - Split your data into a training and testing set\n",
    " - NEVER look at your testing data. Hide it away. Save it for later. Lock it in a drawer!\n",
    " - Your training data helps you to fit your models and select one\n",
    " - Your testing data is used for final evaluation\n",
    " \n",
    "Scikit-learn's train and test split shuffles our data and splits it into two sets. We can also use *stratified sampling* to ensure that both sets have the same distribution of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "training, testing = train_test_split(\n",
    "    df,                # The dataset we want to split\n",
    "    train_size=0.7,    # The proportional size of our training set\n",
    "    stratify=df.label, # The labels are used for stratification\n",
    "    random_state=400   # Use the same random state for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "Now we're ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = pipeline.fit(training.title, training.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What? That was it?! \n",
    "\n",
    "That's right. You've just built a machine learning classifier for clickbait and it was that easy to train. Let's test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict([\"10 things you need to do...\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict([\"French election polls show an early lead for Macron.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introspect our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import print_top_features\n",
    "print_top_features(pipeline, n_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our model\n",
    "\n",
    "Cross-validation is a method of measuring model performance:\n",
    "1. Split the training data into $n$ chunks\n",
    "2. Train the pipeline on all but one of the chunks\n",
    "3. Predict the label of the samples in the remaining chunk\n",
    "4. Repeat this until we have predicted the labels of the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "predicted_labels = cross_val_predict(pipeline, training.title, training.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our predicted labels, we can check our performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plotting import pipeline_performance\n",
    "pipeline_performance(training.label, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always a good idea to inspect those samples that we got wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[training.label != predicted_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model\n",
    "\n",
    "We had pretty high accuracy but can we do better?\n",
    "\n",
    "### Use title and descritption\n",
    "\n",
    "Rather than using just the title, we could use the title and description together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = cross_val_predict(pipeline, training.full_content, training.label)\n",
    "pipeline_performance(training.label, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer Transformer\n",
    "\n",
    "The TfidifVectorizer is an improved version of the CountVectorizer. Words are still transformed into a bag of words but we emphasise the importance of less common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "steps = (\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', LinearSVC())\n",
    ")\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "predicted_labels = cross_val_predict(pipeline, training.full_content, training.label)\n",
    "pipeline_performance(training.label, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace ordinals\n",
    "\n",
    "Earlier we saw that numbers are very common in clickbait articles, they are likely to be a very descriptive feature. However our vocabulary is limited by the numbers that we saw in training. We could replace all numbers by some label so that all numbers are represented equally.\n",
    "\n",
    "All we need to do is write a string preprocessor and pass it to the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def mask_integers(s):\n",
    "    return re.sub(r'\\d+', 'INTMASK', s)\n",
    "\n",
    "steps = (\n",
    "    ('vectorizer', TfidfVectorizer(preprocessor=mask_integers)),\n",
    "    ('classifier', LinearSVC())\n",
    ")\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "predicted_labels = cross_val_predict(pipeline, training.full_content, training.label)\n",
    "pipeline_performance(training.label, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "When we constructed our `LinearSVC` and `CountVectorizer` we used the default model parameters. These additonal parameters (hyperparameters) can be chosen to improve our classifier by performing a grid search. A grid search trains a classifier on every combination of the parameters and analyses their performance. We can then pick the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps = (\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', LinearSVC())\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters we can fiddle:\n",
    " - stop_words - we can ignore certain words (the, a, it,...). scikit-learn has an 'english' stop word vocabulary we can use\n",
    " - ngram_range - in the above example we split sentences into words. We could also try pairs of words.\n",
    " - C - the SVM has a property C that performs regularisation\n",
    " \n",
    "We set up our grid as a dictionary (note we must use the step names so that scikit learn knows which component we are fiddling with):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_params = {\n",
    "    'vectorizer__stop_words': ['english', None],\n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'vectorizer__preprocessor': [mask_integers, None],\n",
    "    'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "gs = GridSearchCV(pipeline, gs_params, n_jobs=-1)\n",
    "gs.fit(training.full_content, training.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_params_)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing generality\n",
    "\n",
    "So far we have selected our model based on the training data alone. It looks like our performance is excellent but we may have overfit to the training data. Does our classifier generalise well?\n",
    "\n",
    "In other words, do we perform as well on data that we have never seen before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = gs.best_estimator_\n",
    "predicted_labels = pipeline.predict(testing.full_content)\n",
    "pipeline_performance(testing.label, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HURRAH!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going into the wild\n",
    "\n",
    "![hunger](https://ronanwills.files.wordpress.com/2015/06/vlcsnap-2015-06-11-23h05m49s76.png?w=625)\n",
    "\n",
    "An excellent feature of scikit-learn is that we can save our classifier using the pickle tool. We can load it later for\n",
    " - data analysis\n",
    " - data provenance\n",
    " - to share with somebody\n",
    " - to provide ML as a service (coming up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'classifiers/clickbait_svc_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(pipeline, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations\n",
    "\n",
    "You've built a clickbait classifer. Head over to `advanced.ipynb` or look through the `webservice` directory to put your pipeline to use.\n",
    "\n",
    "![gatsby](https://media.tenor.co/images/78f5d1acd72e8a66257ea671b4aefd5f/tenor.gif)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
